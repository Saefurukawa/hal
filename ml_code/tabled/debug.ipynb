{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Entries:\n",
      "     Respondent Name Study Name  \\\n",
      "0              20002   Bridge 1   \n",
      "1              20002   Bridge 2   \n",
      "2              20002   Bridge 3   \n",
      "3              20002   Bridge 4   \n",
      "4              20003   Bridge 1   \n",
      "..               ...        ...   \n",
      "159            20045   Bridge 4   \n",
      "160            20046   Bridge 1   \n",
      "161            20046   Bridge 2   \n",
      "162            20046   Bridge 3   \n",
      "163            20046   Bridge 4   \n",
      "\n",
      "                                               Missing  \n",
      "0      crack 14, crack 17, crack 13, crack 10, crack 3  \n",
      "1    crack 14, crack 17, crack 19, crack 1, crack 1...  \n",
      "2    crack 11, crack 14, crack 17, crack 16, crack ...  \n",
      "3    crack 14, crack 15, crack 16, crack 8, crack 4...  \n",
      "4                                           All cracks  \n",
      "..                                                 ...  \n",
      "159  crack 14, crack 19, crack 12, crack 15, crack ...  \n",
      "160  crack 11, crack 17, crack 1, crack 12, crack 1...  \n",
      "161  crack 14, crack 19, crack 15, crack 16, crack ...  \n",
      "162  crack 11, crack 14, crack 17, crack 16, crack ...  \n",
      "163      crack 1, crack 15, crack 16, crack 7, crack 4  \n",
      "\n",
      "[164 rows x 3 columns]\n",
      "Missing data saved to 'missing_entries.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV into a DataFrame\n",
    "df = pd.read_csv(\"data/finalized_data/combined_processed_SaccadeTable.csv\")  # Replace with your CSV file path\n",
    "distance_df = pd.read_csv(\"data/finalized_data/combined_distance_table.csv\")  # Replace with your CSV file path\n",
    "\n",
    "# Define expected bridges and cracks\n",
    "expected_bridges = [f\"Bridge {i}\" for i in range(1, 5)]\n",
    "expected_cracks = [f\"crack {i}\" for i in range(1, 21)]\n",
    "\n",
    "# Group rows by respondent name\n",
    "missing_data = []\n",
    "\n",
    "# Ensure AOI Label and Study Name are in lowercase for consistent matching\n",
    "df['AOI Label'] = df['AOI Label'].str.lower()\n",
    "df['Study Name'] = df['Study Name'].str.lower()\n",
    "\n",
    "# Iterate over each respondent\n",
    "for respondent, group in df.groupby(\"Respondent Name\"):\n",
    "    # Check for each bridge\n",
    "    for bridge in expected_bridges:\n",
    "        bridge_group = group[group['Study Name'] == bridge.lower()]\n",
    "        if bridge_group.empty:\n",
    "            missing_data.append({\"Respondent Name\": respondent, \"Study Name\": bridge, \"Missing\": \"All cracks\"})\n",
    "            continue\n",
    "        \n",
    "        # Check for each crack in the current bridge\n",
    "        present_cracks = set(\n",
    "            bridge_group['AOI Label']\n",
    "            .str.extract(r'(crack \\d+)', expand=False)\n",
    "            .dropna()\n",
    "        )\n",
    "        missing_cracks = set(expected_cracks) - present_cracks\n",
    "        \n",
    "        if missing_cracks:\n",
    "            missing_data.append({\n",
    "                \"Respondent Name\": respondent,\n",
    "                \"Study Name\": bridge,\n",
    "                \"Missing\": \", \".join(missing_cracks)\n",
    "            })\n",
    "\n",
    "# Create a DataFrame of missing entries\n",
    "missing_df = pd.DataFrame(missing_data)\n",
    "\n",
    "# Print missing entries\n",
    "print(\"Missing Entries:\")\n",
    "print(missing_df)\n",
    "\n",
    "# Save to CSV for later reference\n",
    "missing_df.to_csv(\"missing_entries.csv\", index=False)\n",
    "print(\"Missing data saved to 'missing_entries.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Entries:\n",
      "     Respondent Name Study Name  \\\n",
      "0              20002   Bridge 1   \n",
      "1              20002   Bridge 2   \n",
      "2              20002   Bridge 3   \n",
      "3              20002   Bridge 4   \n",
      "4              20003   Bridge 1   \n",
      "..               ...        ...   \n",
      "163            20045   Bridge 4   \n",
      "164            20046   Bridge 1   \n",
      "165            20046   Bridge 2   \n",
      "166            20046   Bridge 3   \n",
      "167            20046   Bridge 4   \n",
      "\n",
      "                                               Missing  \n",
      "0                                              crack 3  \n",
      "1      crack 14, crack 19, crack 15, crack 10, crack 3  \n",
      "2    crack 11, crack 17, crack 4, crack 5, crack 10...  \n",
      "3                          crack 16, crack 4, crack 15  \n",
      "4                                           All cracks  \n",
      "..                                                 ...  \n",
      "163                        crack 16, crack 4, crack 15  \n",
      "164                        crack 3, crack 12, crack 15  \n",
      "165    crack 14, crack 19, crack 15, crack 10, crack 3  \n",
      "166               crack 17, crack 4, crack 5, crack 20  \n",
      "167                        crack 16, crack 4, crack 15  \n",
      "\n",
      "[168 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV into a DataFrame\n",
    "distance_df = pd.read_csv(\"data/finalized_data/combined_distance_table.csv\")  # Replace with your CSV file path\n",
    "\n",
    "# Group rows by respondent name\n",
    "missing_data_2 = []\n",
    "\n",
    "# Ensure AOI Label and Study Name are in lowercase for consistent matching\n",
    "distance_df['Label'] = distance_df['Label'].str.lower()\n",
    "distance_df['Study Name'] = distance_df['Study Name'].str.lower()\n",
    "\n",
    "# Iterate over each respondent\n",
    "for respondent, group in distance_df.groupby(\"Respondent Name\"):\n",
    "    # Check for each bridge\n",
    "    for bridge in expected_bridges:\n",
    "        bridge_group = group[group['Study Name'] == bridge.lower()]\n",
    "        if bridge_group.empty:\n",
    "            missing_data_2.append({\"Respondent Name\": respondent, \"Study Name\": bridge, \"Missing\": \"All cracks\"})\n",
    "            continue\n",
    "        \n",
    "        # Check for each crack in the current bridge\n",
    "        present_cracks = set(\n",
    "            bridge_group['Label']\n",
    "            .str.extract(r'(crack \\d+)', expand=False)\n",
    "            .dropna()\n",
    "        )\n",
    "        missing_cracks = set(expected_cracks) - present_cracks\n",
    "        \n",
    "        if missing_cracks:\n",
    "            missing_data_2.append({\n",
    "                \"Respondent Name\": respondent,\n",
    "                \"Study Name\": bridge,\n",
    "                \"Missing\": \", \".join(missing_cracks)\n",
    "            })\n",
    "\n",
    "# Create a DataFrame of missing entries\n",
    "missing_df_2 = pd.DataFrame(missing_data_2)\n",
    "\n",
    "# Print missing entries\n",
    "print(\"Missing Entries:\")\n",
    "print(missing_df_2)\n",
    "\n",
    "# Save to CSV for later reference\n",
    "missing_df_2.to_csv(\"missing_entries_2.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Entries:\n",
      "    Respondent Name Study Name                                 Missing\n",
      "0             20002   Bridge 2                                crack 19\n",
      "1             20002   Bridge 3                      crack 11, crack 10\n",
      "2             20003   Bridge 1                              All cracks\n",
      "3             20003   Bridge 2                              All cracks\n",
      "4             20003   Bridge 3  crack 11, crack 12, crack 13, crack 10\n",
      "..              ...        ...                                     ...\n",
      "86            20044   Bridge 1                                crack 17\n",
      "87            20044   Bridge 2                                crack 19\n",
      "88            20045   Bridge 2                              All cracks\n",
      "89            20046   Bridge 1             crack 6, crack 12, crack 15\n",
      "90            20046   Bridge 2                                crack 19\n",
      "\n",
      "[91 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV into a DataFrame\n",
    "fixation_df = pd.read_csv(\"data/finalized_data/fixation_aoi.csv\")  # Replace with your CSV file path\n",
    "\n",
    "# Define expected bridges and cracks\n",
    "expected_bridges = [f\"Bridge {i}\" for i in range(1, 5)]\n",
    "expected_cracks = [f\"crack {i}\" for i in range(1, 21)]\n",
    "\n",
    "# Group rows by respondent name\n",
    "missing_data_3 = []\n",
    "\n",
    "# Ensure AOI Label and Study Name are in lowercase for consistent matching\n",
    "fixation_df['Label'] = fixation_df['Label'].str.lower()\n",
    "fixation_df['Study Name'] = fixation_df['Study Name'].str.lower()\n",
    "\n",
    "# Iterate over each respondent\n",
    "for respondent, group in fixation_df.groupby(\"Respondent Name\"):\n",
    "    # Check for each bridge\n",
    "    for bridge in expected_bridges:\n",
    "        bridge_group = group[group['Study Name'] == bridge.lower()]\n",
    "        if bridge_group.empty:\n",
    "            missing_data_3.append({\"Respondent Name\": respondent, \"Study Name\": bridge, \"Missing\": \"All cracks\"})\n",
    "            continue\n",
    "        \n",
    "        # Check for each crack in the current bridge\n",
    "        present_cracks = set(\n",
    "            bridge_group['Label']\n",
    "            .str.extract(r'(crack \\d+)', expand=False)\n",
    "            .dropna()\n",
    "        )\n",
    "        missing_cracks = set(expected_cracks) - present_cracks\n",
    "        \n",
    "        if missing_cracks:\n",
    "            missing_data_3.append({\n",
    "                \"Respondent Name\": respondent,\n",
    "                \"Study Name\": bridge,\n",
    "                \"Missing\": \", \".join(missing_cracks)\n",
    "            })\n",
    "\n",
    "# Create a DataFrame of missing entries\n",
    "missing_df_3 = pd.DataFrame(missing_data_3)\n",
    "\n",
    "# Print missing entries\n",
    "print(\"Missing Entries:\")\n",
    "print(missing_df_3)\n",
    "\n",
    "# Save to CSV for later reference\n",
    "missing_df_3.to_csv(\"missing_entries_3.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows in missing_data but not in missing_data_2:\n",
      "     Respondent Name Study Name  \\\n",
      "0              20002   Bridge 1   \n",
      "1              20002   Bridge 2   \n",
      "2              20002   Bridge 3   \n",
      "3              20002   Bridge 4   \n",
      "6              20003   Bridge 3   \n",
      "..               ...        ...   \n",
      "163            20045   Bridge 4   \n",
      "164            20046   Bridge 1   \n",
      "165            20046   Bridge 2   \n",
      "166            20046   Bridge 3   \n",
      "167            20046   Bridge 4   \n",
      "\n",
      "                                               Missing     _merge  \n",
      "0                                              crack 3  left_only  \n",
      "1      crack 14, crack 19, crack 15, crack 10, crack 3  left_only  \n",
      "2    crack 11, crack 17, crack 4, crack 5, crack 10...  left_only  \n",
      "3                          crack 16, crack 4, crack 15  left_only  \n",
      "6    crack 11, crack 17, crack 12, crack 13, crack ...  left_only  \n",
      "..                                                 ...        ...  \n",
      "163                        crack 16, crack 4, crack 15  left_only  \n",
      "164                        crack 3, crack 12, crack 15  left_only  \n",
      "165    crack 14, crack 19, crack 15, crack 10, crack 3  left_only  \n",
      "166               crack 17, crack 4, crack 5, crack 20  left_only  \n",
      "167                        crack 16, crack 4, crack 15  left_only  \n",
      "\n",
      "[135 rows x 4 columns]\n",
      "\n",
      "Rows in missing_data_2 but not in missing_data:\n",
      "     Respondent Name Study Name  \\\n",
      "168            20002   Bridge 2   \n",
      "169            20002   Bridge 3   \n",
      "170            20003   Bridge 3   \n",
      "171            20004   Bridge 1   \n",
      "172            20004   Bridge 2   \n",
      "173            20005   Bridge 2   \n",
      "174            20006   Bridge 2   \n",
      "175            20007   Bridge 3   \n",
      "176            20008   Bridge 2   \n",
      "177            20009   Bridge 1   \n",
      "178            20009   Bridge 2   \n",
      "179            20009   Bridge 3   \n",
      "180            20010   Bridge 2   \n",
      "181            20012   Bridge 1   \n",
      "182            20012   Bridge 2   \n",
      "183            20013   Bridge 1   \n",
      "184            20013   Bridge 2   \n",
      "185            20015   Bridge 2   \n",
      "186            20015   Bridge 3   \n",
      "187            20015   Bridge 4   \n",
      "188            20016   Bridge 1   \n",
      "189            20016   Bridge 2   \n",
      "190            20017   Bridge 1   \n",
      "191            20017   Bridge 2   \n",
      "192            20017   Bridge 3   \n",
      "193            20017   Bridge 4   \n",
      "194            20019   Bridge 1   \n",
      "195            20019   Bridge 2   \n",
      "196            20019   Bridge 4   \n",
      "197            20020   Bridge 3   \n",
      "198            20021   Bridge 1   \n",
      "199            20022   Bridge 1   \n",
      "200            20023   Bridge 2   \n",
      "201            20024   Bridge 2   \n",
      "202            20025   Bridge 2   \n",
      "203            20025   Bridge 4   \n",
      "204            20026   Bridge 2   \n",
      "205            20030   Bridge 2   \n",
      "206            20031   Bridge 2   \n",
      "207            20031   Bridge 4   \n",
      "208            20032   Bridge 2   \n",
      "209            20034   Bridge 1   \n",
      "210            20036   Bridge 2   \n",
      "211            20037   Bridge 2   \n",
      "212            20038   Bridge 1   \n",
      "213            20038   Bridge 2   \n",
      "214            20039   Bridge 2   \n",
      "215            20039   Bridge 3   \n",
      "216            20039   Bridge 4   \n",
      "217            20041   Bridge 1   \n",
      "218            20041   Bridge 3   \n",
      "219            20042   Bridge 2   \n",
      "220            20043   Bridge 1   \n",
      "221            20043   Bridge 2   \n",
      "222            20044   Bridge 1   \n",
      "223            20044   Bridge 2   \n",
      "224            20046   Bridge 1   \n",
      "225            20046   Bridge 2   \n",
      "\n",
      "                                               Missing      _merge  \n",
      "168                                           crack 19  right_only  \n",
      "169                                 crack 11, crack 10  right_only  \n",
      "170             crack 11, crack 12, crack 13, crack 10  right_only  \n",
      "171                                 crack 11, crack 15  right_only  \n",
      "172                                           crack 19  right_only  \n",
      "173                                           crack 19  right_only  \n",
      "174                                           crack 19  right_only  \n",
      "175                                            crack 5  right_only  \n",
      "176                                           crack 19  right_only  \n",
      "177                                         All cracks  right_only  \n",
      "178                                           crack 19  right_only  \n",
      "179                                 crack 19, crack 18  right_only  \n",
      "180                                           crack 19  right_only  \n",
      "181  crack 19, crack 12, crack 15, crack 16, crack ...  right_only  \n",
      "182                        crack 7, crack 19, crack 10  right_only  \n",
      "183                                           crack 18  right_only  \n",
      "184                                           crack 19  right_only  \n",
      "185                                           crack 19  right_only  \n",
      "186  crack 11, crack 14, crack 17, crack 15, crack ...  right_only  \n",
      "187                                 crack 17, crack 13  right_only  \n",
      "188                                 crack 16, crack 15  right_only  \n",
      "189                                           crack 19  right_only  \n",
      "190                                 crack 13, crack 15  right_only  \n",
      "191                                           crack 19  right_only  \n",
      "192                       crack 16, crack 17, crack 14  right_only  \n",
      "193                                            crack 5  right_only  \n",
      "194                                         All cracks  right_only  \n",
      "195                                         All cracks  right_only  \n",
      "196                                           crack 13  right_only  \n",
      "197                                           crack 14  right_only  \n",
      "198                                           crack 14  right_only  \n",
      "199                                           crack 20  right_only  \n",
      "200                                           crack 19  right_only  \n",
      "201                                           crack 19  right_only  \n",
      "202                                         All cracks  right_only  \n",
      "203                 crack 7, crack 9, crack 8, crack 6  right_only  \n",
      "204                                 crack 17, crack 19  right_only  \n",
      "205                                            crack 1  right_only  \n",
      "206                                 crack 19, crack 10  right_only  \n",
      "207                                 crack 13, crack 10  right_only  \n",
      "208                                           crack 10  right_only  \n",
      "209             crack 14, crack 19, crack 20, crack 18  right_only  \n",
      "210                                           crack 19  right_only  \n",
      "211                                           crack 19  right_only  \n",
      "212             crack 14, crack 19, crack 20, crack 18  right_only  \n",
      "213                                           crack 19  right_only  \n",
      "214                                           crack 19  right_only  \n",
      "215  crack 14, crack 17, crack 12, crack 15, crack ...  right_only  \n",
      "216                                           crack 17  right_only  \n",
      "217                                           crack 17  right_only  \n",
      "218                       crack 16, crack 19, crack 18  right_only  \n",
      "219                                           crack 19  right_only  \n",
      "220                                           crack 17  right_only  \n",
      "221                                           crack 19  right_only  \n",
      "222                                           crack 17  right_only  \n",
      "223                                           crack 19  right_only  \n",
      "224                        crack 6, crack 12, crack 15  right_only  \n",
      "225                                           crack 19  right_only  \n",
      "\n",
      "Differences saved to 'unique_to_missing_data.csv' and 'unique_to_missing_data_2.csv'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Assuming missing_data and missing_data_2 are pandas DataFrames\n",
    "# Replace these with your actual data loading code if necessary\n",
    "missing_data = pd.read_csv(\"missing_entries.csv\")\n",
    "missing_data_2 = pd.read_csv(\"missing_entries_2.csv\")\n",
    "missing_data_3 = pd.read_csv(\"missing_entries_3.csv\")\n",
    "\n",
    "# Perform an outer join to identify differences\n",
    "comparison = pd.merge(\n",
    "    missing_data_2, missing_data_3,\n",
    "    on=[\"Respondent Name\", \"Study Name\", \"Missing\"],  # Columns to compare\n",
    "    how=\"outer\",\n",
    "    indicator=True  # This will add a column to indicate the source of the row\n",
    ")\n",
    "\n",
    "# Find rows unique to missing_data\n",
    "unique_to_missing_data = comparison[comparison[\"_merge\"] == \"left_only\"]\n",
    "\n",
    "# Find rows unique to missing_data_2\n",
    "unique_to_missing_data_2 = comparison[comparison[\"_merge\"] == \"right_only\"]\n",
    "\n",
    "# Print the results\n",
    "print(\"Rows in missing_data but not in missing_data_2:\")\n",
    "print(unique_to_missing_data)\n",
    "\n",
    "print(\"\\nRows in missing_data_2 but not in missing_data:\")\n",
    "print(unique_to_missing_data_2)\n",
    "\n",
    "# Save results for further inspection\n",
    "unique_to_missing_data.to_csv(\"unique_to_missing_data.csv\", index=False)\n",
    "unique_to_missing_data_2.to_csv(\"unique_to_missing_data_2.csv\", index=False)\n",
    "\n",
    "print(\"\\nDifferences saved to 'unique_to_missing_data.csv' and 'unique_to_missing_data_2.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows in distance_df but not in df:\n",
      "     Study Name  Respondent Name          Label  Distance  \\\n",
      "3      Bridge 3            20002    Crack 6 Hit       2.0   \n",
      "9      Bridge 3            20002  Crack 14 Miss       2.0   \n",
      "11     Bridge 3            20002   Crack 16 Hit       2.0   \n",
      "12     Bridge 3            20002   Crack 18 Hit       2.0   \n",
      "22     Bridge 3            20003  Crack 15 Miss       3.0   \n",
      "...         ...              ...            ...       ...   \n",
      "2117   Bridge 2            20046    Crack 2 Hit       2.0   \n",
      "2119   Bridge 2            20046    Crack 5 Hit       2.0   \n",
      "2120   Bridge 2            20046   Crack 6 Miss       4.0   \n",
      "2123   Bridge 2            20046    Crack 9 Hit       2.0   \n",
      "2127   Bridge 2            20046   Crack 16 HIt       3.0   \n",
      "\n",
      "      Saccade Duration Mean  Saccade Duration Std  Saccade Duration Median  \\\n",
      "3                       NaN                   NaN                      NaN   \n",
      "9                       NaN                   NaN                      NaN   \n",
      "11                      NaN                   NaN                      NaN   \n",
      "12                      NaN                   NaN                      NaN   \n",
      "22                      NaN                   NaN                      NaN   \n",
      "...                     ...                   ...                      ...   \n",
      "2117                    NaN                   NaN                      NaN   \n",
      "2119                    NaN                   NaN                      NaN   \n",
      "2120                    NaN                   NaN                      NaN   \n",
      "2123                    NaN                   NaN                      NaN   \n",
      "2127                    NaN                   NaN                      NaN   \n",
      "\n",
      "      Saccade Amplitude Mean  Saccade Amplitude Std  Saccade Amplitude Median  \\\n",
      "3                        NaN                    NaN                       NaN   \n",
      "9                        NaN                    NaN                       NaN   \n",
      "11                       NaN                    NaN                       NaN   \n",
      "12                       NaN                    NaN                       NaN   \n",
      "22                       NaN                    NaN                       NaN   \n",
      "...                      ...                    ...                       ...   \n",
      "2117                     NaN                    NaN                       NaN   \n",
      "2119                     NaN                    NaN                       NaN   \n",
      "2120                     NaN                    NaN                       NaN   \n",
      "2123                     NaN                    NaN                       NaN   \n",
      "2127                     NaN                    NaN                       NaN   \n",
      "\n",
      "      Saccade Peak Velocity Mean  Saccade Peak Velocity Std  \\\n",
      "3                            NaN                        NaN   \n",
      "9                            NaN                        NaN   \n",
      "11                           NaN                        NaN   \n",
      "12                           NaN                        NaN   \n",
      "22                           NaN                        NaN   \n",
      "...                          ...                        ...   \n",
      "2117                         NaN                        NaN   \n",
      "2119                         NaN                        NaN   \n",
      "2120                         NaN                        NaN   \n",
      "2123                         NaN                        NaN   \n",
      "2127                         NaN                        NaN   \n",
      "\n",
      "      Saccade Peak Velocity Median  Saccade Peak Acceleration Mean  \\\n",
      "3                              NaN                             NaN   \n",
      "9                              NaN                             NaN   \n",
      "11                             NaN                             NaN   \n",
      "12                             NaN                             NaN   \n",
      "22                             NaN                             NaN   \n",
      "...                            ...                             ...   \n",
      "2117                           NaN                             NaN   \n",
      "2119                           NaN                             NaN   \n",
      "2120                           NaN                             NaN   \n",
      "2123                           NaN                             NaN   \n",
      "2127                           NaN                             NaN   \n",
      "\n",
      "      Saccade Peak Acceleration Std  Saccade Peak Acceleration Median  \\\n",
      "3                               NaN                               NaN   \n",
      "9                               NaN                               NaN   \n",
      "11                              NaN                               NaN   \n",
      "12                              NaN                               NaN   \n",
      "22                              NaN                               NaN   \n",
      "...                             ...                               ...   \n",
      "2117                            NaN                               NaN   \n",
      "2119                            NaN                               NaN   \n",
      "2120                            NaN                               NaN   \n",
      "2123                            NaN                               NaN   \n",
      "2127                            NaN                               NaN   \n",
      "\n",
      "      Saccade Peak Deceleration Mean  Saccade Peak Deceleration Std  \\\n",
      "3                                NaN                            NaN   \n",
      "9                                NaN                            NaN   \n",
      "11                               NaN                            NaN   \n",
      "12                               NaN                            NaN   \n",
      "22                               NaN                            NaN   \n",
      "...                              ...                            ...   \n",
      "2117                             NaN                            NaN   \n",
      "2119                             NaN                            NaN   \n",
      "2120                             NaN                            NaN   \n",
      "2123                             NaN                            NaN   \n",
      "2127                             NaN                            NaN   \n",
      "\n",
      "      Saccade Peak Deceleration Median  Saccade Count  \n",
      "3                                  NaN            NaN  \n",
      "9                                  NaN            NaN  \n",
      "11                                 NaN            NaN  \n",
      "12                                 NaN            NaN  \n",
      "22                                 NaN            NaN  \n",
      "...                                ...            ...  \n",
      "2117                               NaN            NaN  \n",
      "2119                               NaN            NaN  \n",
      "2120                               NaN            NaN  \n",
      "2123                               NaN            NaN  \n",
      "2127                               NaN            NaN  \n",
      "\n",
      "[627 rows x 20 columns]\n",
      "Unique rows saved to 'unique_to_distance.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSVs into DataFrames\n",
    "df = pd.read_csv(\"data/finalized_data/combined_processed_SaccadeTable.csv\")\n",
    "distance_df = pd.read_csv(\"data/finalized_data/combined_distance_table.csv\")\n",
    "\n",
    "# Perform an outer join to find rows in `distance_df` that are not in `df`\n",
    "comparison = pd.merge(\n",
    "    distance_df,\n",
    "    df,\n",
    "    on=[\"Study Name\", \"Respondent Name\", \"Label\"],  # Columns to compare\n",
    "    how=\"left\",\n",
    "    indicator=True  # Add a column to indicate source\n",
    ")\n",
    "\n",
    "# Filter rows that exist only in `distance_df`\n",
    "only_in_distance = comparison[comparison[\"_merge\"] == \"left_only\"]\n",
    "\n",
    "# Drop the `_merge` column to clean up the output\n",
    "only_in_distance = only_in_distance.drop(columns=[\"_merge\"])\n",
    "\n",
    "# Print the rows that are unique to `distance_df`\n",
    "print(\"Rows in distance_df but not in df:\")\n",
    "print(only_in_distance)\n",
    "\n",
    "# Save the output for further inspection if needed\n",
    "only_in_distance.to_csv(\"unique_to_distance.csv\", index=False)\n",
    "print(\"Unique rows saved to 'unique_to_distance.csv'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15 (main, Oct  3 2024, 02:33:33) [Clang 14.0.6 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2c229cd6ff87d19ea7d47541bfa1b62a250f965db58eb2430f02bd66bc83489c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
