{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1818ded70>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "torch.manual_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label distribution after SMOTE:\n",
      "Counter({0: 1232, 1: 1232})\n"
     ]
    }
   ],
   "source": [
    "# CSVDataset Class with On-the-Fly Normalization\n",
    "from collections import Counter\n",
    "class CSVDataset(Dataset):\n",
    "    def __init__(self, csv_file):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.labels = self.data.iloc[:, 0].values  # First column as labels\n",
    "        self.features = self.data.iloc[:, 1:].values  # Remaining columns as features\n",
    "        \n",
    "        # Handle NaN values\n",
    "        imputer = SimpleImputer(strategy=\"mean\")\n",
    "        self.features = imputer.fit_transform(self.features)\n",
    "        \n",
    "        # Normalize features\n",
    "        self.scaler = StandardScaler()\n",
    "        self.features = self.scaler.fit_transform(self.features)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        features = self.features[idx]\n",
    "        label = self.labels[idx]\n",
    "        return torch.tensor(features, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n",
    "    \n",
    "# Load data\n",
    "csv_file = \"data/merged_data/filtered_merged.csv\"  \n",
    "dataset = CSVDataset(csv_file)\n",
    "\n",
    "# Split dataset\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Prepare train and test features/labels for SMOTE\n",
    "X_train = np.array([train_dataset[i][0].numpy() for i in range(len(train_dataset))])\n",
    "y_train = np.array([train_dataset[i][1].item() for i in range(len(train_dataset))])\n",
    "\n",
    "# Apply SMOTE to balance the dataset\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Check label distribution after SMOTE\n",
    "print(\"Label distribution after SMOTE:\")\n",
    "print(Counter(y_train_resampled))\n",
    "\n",
    "# Create PyTorch datasets\n",
    "train_dataset = [(torch.tensor(X_train_resampled[i], dtype=torch.float32),\n",
    "                  torch.tensor(y_train_resampled[i], dtype=torch.long)) \n",
    "                 for i in range(len(y_train_resampled))]\n",
    "test_dataset = [(torch.tensor(test_dataset[i][0].numpy(), dtype=torch.float32),\n",
    "                 test_dataset[i][1]) \n",
    "                for i in range(len(test_dataset))]\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(torch.nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(n_inputs, n_outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.linear(x))  # Apply sigmoid here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "n_inputs = X_train.shape[1]   # Number of features (excluding the label)\n",
    "# features, label = dataset[0]\n",
    "\n",
    "# # Determine the number of input features\n",
    "# n_inputs = features.shape[0]\n",
    "n_outputs = 1  # Binary classification\n",
    "log_regr = LogisticRegression(n_inputs, n_outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 0.6927, Accuracy: 56.39%\n",
      "Epoch 2/100, Loss: 0.7527, Accuracy: 56.89%\n",
      "Epoch 3/100, Loss: 0.6637, Accuracy: 58.90%\n",
      "Epoch 4/100, Loss: 0.7201, Accuracy: 60.40%\n",
      "Epoch 5/100, Loss: 0.7396, Accuracy: 60.90%\n",
      "Epoch 6/100, Loss: 0.6597, Accuracy: 62.91%\n",
      "Epoch 7/100, Loss: 0.6381, Accuracy: 64.16%\n",
      "Epoch 8/100, Loss: 0.7030, Accuracy: 64.91%\n",
      "Epoch 9/100, Loss: 0.6705, Accuracy: 66.17%\n",
      "Epoch 10/100, Loss: 0.6300, Accuracy: 67.17%\n",
      "Epoch 11/100, Loss: 0.6615, Accuracy: 68.42%\n",
      "Epoch 12/100, Loss: 0.6113, Accuracy: 69.42%\n",
      "Epoch 13/100, Loss: 0.6363, Accuracy: 70.18%\n",
      "Epoch 14/100, Loss: 0.6639, Accuracy: 70.18%\n",
      "Epoch 15/100, Loss: 0.6784, Accuracy: 70.43%\n",
      "Epoch 16/100, Loss: 0.5959, Accuracy: 70.93%\n",
      "Epoch 17/100, Loss: 0.7016, Accuracy: 71.68%\n",
      "Epoch 18/100, Loss: 0.6163, Accuracy: 71.93%\n",
      "Epoch 19/100, Loss: 0.5885, Accuracy: 72.18%\n",
      "Epoch 20/100, Loss: 0.5671, Accuracy: 72.43%\n",
      "Epoch 21/100, Loss: 0.6068, Accuracy: 72.43%\n",
      "Epoch 22/100, Loss: 0.6065, Accuracy: 72.43%\n",
      "Epoch 23/100, Loss: 0.5297, Accuracy: 73.18%\n",
      "Epoch 24/100, Loss: 0.5792, Accuracy: 73.68%\n",
      "Epoch 25/100, Loss: 0.5967, Accuracy: 73.68%\n",
      "Epoch 26/100, Loss: 0.5805, Accuracy: 73.93%\n",
      "Epoch 27/100, Loss: 0.5621, Accuracy: 73.93%\n",
      "Epoch 28/100, Loss: 0.6494, Accuracy: 73.68%\n",
      "Epoch 29/100, Loss: 0.5808, Accuracy: 73.68%\n",
      "Epoch 30/100, Loss: 0.6093, Accuracy: 74.19%\n",
      "Epoch 31/100, Loss: 0.6170, Accuracy: 74.19%\n",
      "Epoch 32/100, Loss: 0.5849, Accuracy: 74.44%\n",
      "Epoch 33/100, Loss: 0.5692, Accuracy: 74.44%\n",
      "Epoch 34/100, Loss: 0.5494, Accuracy: 74.69%\n",
      "Epoch 35/100, Loss: 0.5197, Accuracy: 74.69%\n",
      "Epoch 36/100, Loss: 0.6422, Accuracy: 74.94%\n",
      "Epoch 37/100, Loss: 0.5599, Accuracy: 75.69%\n",
      "Epoch 38/100, Loss: 0.5227, Accuracy: 75.44%\n",
      "Epoch 39/100, Loss: 0.6136, Accuracy: 75.44%\n",
      "Epoch 40/100, Loss: 0.5373, Accuracy: 75.44%\n",
      "Epoch 41/100, Loss: 0.5451, Accuracy: 75.69%\n",
      "Epoch 42/100, Loss: 0.5366, Accuracy: 75.69%\n",
      "Epoch 43/100, Loss: 0.5854, Accuracy: 75.69%\n",
      "Epoch 44/100, Loss: 0.5593, Accuracy: 76.44%\n",
      "Epoch 45/100, Loss: 0.5997, Accuracy: 76.69%\n",
      "Epoch 46/100, Loss: 0.5478, Accuracy: 76.69%\n",
      "Epoch 47/100, Loss: 0.6036, Accuracy: 76.44%\n",
      "Epoch 48/100, Loss: 0.5245, Accuracy: 76.44%\n",
      "Epoch 49/100, Loss: 0.5718, Accuracy: 76.44%\n",
      "Epoch 50/100, Loss: 0.5096, Accuracy: 76.94%\n",
      "Epoch 51/100, Loss: 0.5412, Accuracy: 77.19%\n",
      "Epoch 52/100, Loss: 0.5247, Accuracy: 77.19%\n",
      "Epoch 53/100, Loss: 0.5068, Accuracy: 76.94%\n",
      "Epoch 54/100, Loss: 0.4600, Accuracy: 76.94%\n",
      "Epoch 55/100, Loss: 0.4624, Accuracy: 76.94%\n",
      "Epoch 56/100, Loss: 0.5555, Accuracy: 76.94%\n",
      "Epoch 57/100, Loss: 0.5098, Accuracy: 76.94%\n",
      "Epoch 58/100, Loss: 0.4891, Accuracy: 77.19%\n",
      "Epoch 59/100, Loss: 0.4660, Accuracy: 77.19%\n",
      "Epoch 60/100, Loss: 0.4192, Accuracy: 77.44%\n",
      "Epoch 61/100, Loss: 0.5228, Accuracy: 77.44%\n",
      "Epoch 62/100, Loss: 0.4721, Accuracy: 77.69%\n",
      "Epoch 63/100, Loss: 0.4993, Accuracy: 77.69%\n",
      "Epoch 64/100, Loss: 0.5140, Accuracy: 77.69%\n",
      "Epoch 65/100, Loss: 0.4606, Accuracy: 77.69%\n",
      "Epoch 66/100, Loss: 0.5138, Accuracy: 77.69%\n",
      "Epoch 67/100, Loss: 0.5174, Accuracy: 77.94%\n",
      "Epoch 68/100, Loss: 0.4901, Accuracy: 77.94%\n",
      "Epoch 69/100, Loss: 0.5167, Accuracy: 77.94%\n",
      "Epoch 70/100, Loss: 0.5056, Accuracy: 77.94%\n",
      "Epoch 71/100, Loss: 0.4773, Accuracy: 77.94%\n",
      "Epoch 72/100, Loss: 0.4196, Accuracy: 77.94%\n",
      "Epoch 73/100, Loss: 0.4857, Accuracy: 78.20%\n",
      "Epoch 74/100, Loss: 0.5077, Accuracy: 78.20%\n",
      "Epoch 75/100, Loss: 0.4217, Accuracy: 78.20%\n",
      "Epoch 76/100, Loss: 0.4597, Accuracy: 78.45%\n",
      "Epoch 77/100, Loss: 0.5162, Accuracy: 78.45%\n",
      "Epoch 78/100, Loss: 0.5162, Accuracy: 78.20%\n",
      "Epoch 79/100, Loss: 0.4355, Accuracy: 78.45%\n",
      "Epoch 80/100, Loss: 0.5350, Accuracy: 78.45%\n",
      "Epoch 81/100, Loss: 0.5207, Accuracy: 78.70%\n",
      "Epoch 82/100, Loss: 0.4301, Accuracy: 78.95%\n",
      "Epoch 83/100, Loss: 0.4400, Accuracy: 78.95%\n",
      "Epoch 84/100, Loss: 0.5404, Accuracy: 78.95%\n",
      "Epoch 85/100, Loss: 0.5890, Accuracy: 78.95%\n",
      "Epoch 86/100, Loss: 0.4570, Accuracy: 78.95%\n",
      "Epoch 87/100, Loss: 0.5023, Accuracy: 78.95%\n",
      "Epoch 88/100, Loss: 0.4793, Accuracy: 78.95%\n",
      "Epoch 89/100, Loss: 0.4829, Accuracy: 78.95%\n",
      "Epoch 90/100, Loss: 0.5202, Accuracy: 78.95%\n",
      "Epoch 91/100, Loss: 0.5007, Accuracy: 78.95%\n",
      "Epoch 92/100, Loss: 0.4590, Accuracy: 78.95%\n",
      "Epoch 93/100, Loss: 0.4212, Accuracy: 78.95%\n",
      "Epoch 94/100, Loss: 0.4886, Accuracy: 78.95%\n",
      "Epoch 95/100, Loss: 0.4041, Accuracy: 78.95%\n",
      "Epoch 96/100, Loss: 0.4447, Accuracy: 78.95%\n",
      "Epoch 97/100, Loss: 0.4149, Accuracy: 78.95%\n",
      "Epoch 98/100, Loss: 0.4578, Accuracy: 78.70%\n",
      "Epoch 99/100, Loss: 0.5235, Accuracy: 78.70%\n",
      "Epoch 100/100, Loss: 0.4564, Accuracy: 78.95%\n",
      "Training Complete!\n"
     ]
    }
   ],
   "source": [
    "# Define optimizer and loss function\n",
    "optimizer = torch.optim.SGD(log_regr.parameters(), lr=0.0003)\n",
    "criterion = torch.nn.BCELoss()  # Binary Cross-Entropy Loss\n",
    "\n",
    "# Training and Evaluation Loop\n",
    "epochs = 100\n",
    "Loss = []\n",
    "acc = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Training Loop\n",
    "    log_regr.train()\n",
    "    for features, labels in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = log_regr(features).squeeze()  # Squeeze to match shape for BCELoss\n",
    "        labels = labels.float()  # Convert labels to float for BCELoss\n",
    "        # print(outputs)\n",
    "        # Debugging shapes\n",
    "        # print(f\"Outputs shape: {outputs.shape}, Labels shape: {labels.shape}\")\n",
    "        labels = labels.float() \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Evaluation Loop\n",
    "    log_regr.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for features, labels in test_dataloader:\n",
    "            outputs = log_regr(features).squeeze()\n",
    "            predicted = (outputs > 0.5).float()  # Apply threshold for binary classification\n",
    "            correct += (predicted == labels.float()).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    Loss.append(loss.item())\n",
    "    acc.append(accuracy)\n",
    "    print(f'Epoch {epoch + 1}/{epochs}, Loss: {loss.item():.4f}, Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "# Save final metrics\n",
    "print(\"Training Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[304   0]\n",
      " [ 95   0]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.76      1.00      0.86       304\n",
      "     Class 1       0.00      0.00      0.00        95\n",
      "\n",
      "    accuracy                           0.76       399\n",
      "   macro avg       0.38      0.50      0.43       399\n",
      "weighted avg       0.58      0.76      0.66       399\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/saefurukawa/opt/anaconda3/envs/pai/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/saefurukawa/opt/anaconda3/envs/pai/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/saefurukawa/opt/anaconda3/envs/pai/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# log_regr.load_state_dict(torch.load('logistic_regression_model.pth'))\n",
    "log_regr.eval()\n",
    "\n",
    "# Evaluate the model on the evaluation dataset\n",
    "log_regr.eval()\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for features, labels in test_dataloader:\n",
    "        outputs = log_regr(features)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        all_labels.extend(labels.numpy())\n",
    "        all_predictions.extend(predicted.numpy())\n",
    "\n",
    "# Compute metrics\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(all_labels, all_predictions))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_labels, all_predictions, target_names=[\"Class 0\", \"Class 1\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15 (main, Oct  3 2024, 02:33:33) [Clang 14.0.6 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2c229cd6ff87d19ea7d47541bfa1b62a250f965db58eb2430f02bd66bc83489c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
