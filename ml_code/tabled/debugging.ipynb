{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we filter the original csv to keep rows that are relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input directory and output file\n",
    "file_path = 'data/saccade/SaccadeTable_easy1_second.csv'  # Update with your directory path\n",
    "output_file = 'debugging.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Respondent 20022 still present after filtering AOI Label at data/saccade/SaccadeTable_easy1_second.csv\n",
      "{'Saccade Amplitude': ['NA', 'NA', '0.92544799', '3.4434758', 'NA', '0.57982414', '1.21623425', 'NA', 'NA', 'NA', '1.27328824', '2.01638746', 'NA', '7.21633971', 'NA', '0.73965184', 'NA', '0.49975736', '0.67998746', 'NA', 'NA', '6.10021627', '1.90906727', '4.18815397', 'NA', 'NA', 'NA', '1.29234572', 'NA', 'NA', 'NA', '3.75715878', '0.59761714', 'NA', 'NA', '17.78168628', '3.26966941', 'NA', '1.04280893', 'NA', 'NA', '2.70374568', '19.46891175', '20.21086205', 'NA', 'NA', '2.06470267', '0.23464685', 'NA', 'NA', 'NA', '1.81220047', '5.43609861', '8.21690475', 'NA', 'NA', '0.52556569', 'NA', 'NA', '1.74151635', '0.20300514', '0.03622319', '2.27144317', 'NA', 'NA', '1.53728133', '0.16182597', '0.75168673', '0.7918555', '0.44034693', '0.58129926', '1.22715848', 'NA', '0.95569841', '0.41086927', '0.36692916', 'NA', '0.63776447', '1.30550218', 'NA', 'NA', 'NA', '0.60230954', '2.4378683', '0.49283117', '0.44515979', '4.00636411', '8.04743644', '1.45578114', '0.40965731', 'NA', '1.19978922', 'NA', '0.49505392', '1.10291694', 'NA', 'NA', '1.04483094', '0.75672704', 'NA', '0.26026029', '0.0324779', 'NA', '11.22669186', 'NA', '7.43824803', '2.61505198', 'NA', 'NA', 'NA', 'NA', '2.32200292', '3.1405882', 'NA', '7.20465149', 'NA', '0.39793024', '0.83071186', 'NA', '3.18742803', 'NA', 'NA', '0.79616777', 'NA', '0.56108985', '6.65958836', '8.96505055', '0.3742669', '6.45904256', 'NA', 'NA', 'NA', '2.55495539', '4.78104418', 'NA', 'NA', 'NA', '2.38529791', '0.39217058', 'NA', '2.10898691', '4.60852169', 'NA', '1.01051801', 'NA', 'NA', 'NA', 'NA', '0.32924961', 'NA', 'NA', '1.73140644', 'NA', '6.85364007', 'NA', '1.06817568', '0.63373604', '0.54168329', '2.35391446', '0.45585586', 'NA', '0.71943738', '0.51350879', '1.3302369', '0.25069595', 'NA', '0.57969995', '0.62552197', '1.14869483', '1.02530618', 'NA', '0.3737878', '0.77983378', '0.69694828', 'NA', 'NA', 'NA', '1.08100495', '0.28675287', 'NA', 'NA', '4.19351675', '4.61133191', '5.74700486', '17.03766385', '17.36328475'], 'Saccade Peak Velocity': [32.40932, 36.9344, 65.47441, 151.65123, 40.51984, 367.77292, 77.03458, 32.10597, 42.54744, 33.19513, 76.05302, 120.35271, 30.90542, 267.00667, 31.53038, 52.50962, 150.33308, 36.08449, 57.02319, 37.31717, 74.67387, 265.38988, 344.52199, 132.99297, 45.72627, 30.07262, 42.54372, 105.47951, 39.20085, 32.99449, 152.98485, 138.89801, 39.26139, 35.66654, 31.82442, 360.83623, 168.15741, 53.03137, 69.79805, 49.52125, 151.29143, 114.62653, 420.70214, 355.37168, 38.04823, 78.56192, 55.95856, 45.57818, 36.68873, 42.8197, 43.79209, 283.15626, 196.47301, 238.12309, 38.44946, 42.61751, 38.60963, 49.61149, 42.07578, 157.90213, 83.89693, 104.97626, 118.24607, 57.70442, 63.20303, 188.08358, 56.60827, 102.61498, 130.46328, 99.63656, 142.47913, 83.26522, 63.36187, 57.00242, 42.72212, 47.53355, 43.13815, 38.52863, 101.15314, 35.40794, 66.67091, 78.18524, 42.06123, 146.70089, 44.84074, 34.92948, 215.40353, 295.27976, 90.24036, 33.85391, 37.04617, 71.68204, 30.00941, 38.08584, 61.3014, 51.77835, 38.54852, 94.49423, 58.28578, 44.54112, 119.69268, 90.19462, 102.23033, 227.42655, 40.38196, 261.96627, 132.81413, 30.08046, 32.64301, 46.03387, 32.35397, 116.74247, 152.06297, 33.96027, 241.0432, 46.31956, 41.37878, 54.0309, 229.84484, 111.44912, 42.32127, 58.2272, 113.09178, 61.79245, 64.36134, 257.37177, 327.44531, 46.50441, 207.43013, 97.00967, 43.68382, 32.51096, 149.9901, 202.82362, 37.13864, 51.29784, 205.24973, 100.96856, 66.61986, 43.36975, 147.07619, 179.23054, 76.42225, 59.3664, 48.88739, 42.92166, 94.31492, 95.9676, 41.04575, 30.97775, 30.53701, 102.97847, 39.72493, 242.80646, 30.85028, 65.39382, 41.1909, 53.30088, 145.62673, 43.10412, 38.00972, 45.67733, 47.4888, 89.24635, 46.41293, 41.54179, 50.9196, 31.63321, 69.2818, 48.64736, 87.51952, 100.94023, 68.88347, 45.89738, 68.55836, 89.73757, 68.34195, 57.61525, 70.13204, 88.25245, 55.70043, 157.3143, 168.98366, 109.46971, 496.85528, 613.77522]}\n",
      "{'Saccade Amplitude': [2.876303968224299, 4.132860361167233], 'Saccade Peak Velocity': [99.33965924731181, 91.54824223692806]}\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty list to hold all rows for the combined DataFrame\n",
    "combined_rows = []\n",
    "\n",
    "# Iterate over all CSV files in the specified directory\n",
    "\n",
    "# Load each CSV file into a DataFrame, skipping the first 6 rows\n",
    "df = pd.read_csv(file_path, skiprows=6, keep_default_na=False)\n",
    "\n",
    "# Drop specific columns if they exist in the DataFrame\n",
    "columns_to_drop = [\"Respondent Gender\", \"Respondent Age\", \"Respondent Group\", \"Stimulus Label\"]\n",
    "df = df.drop([col for col in columns_to_drop if col in df.columns], axis='columns')\n",
    "\n",
    "#Drop participants where ET is lower than 85\n",
    "df.drop(df[(df['Study Name'].str.contains(\"easy 1\", na=False)) & (df[\"Respondent Name\"]==20020)].index, inplace=True)\n",
    "\n",
    "# Drop rows based on conditions\n",
    "df = df[df[\"Gaze Calibration\"] != \"Poor\"]\n",
    "df = df[df[\"AOI Label\"] != \"NA\"]\n",
    "# Check again if \"Respondent Name\" still contains 20022\n",
    "if (df[\"Respondent Name\"] == 20022).any():\n",
    "    print(\"Respondent 20022 still present after filtering AOI Label at\", file_path)\n",
    "else:\n",
    "    print(\"Respondent 20022 not present after filtering AOI Label.\")\n",
    "\n",
    "    \n",
    "# Initialize a dictionary to collect participant-specific data for calculating statistics\n",
    "participants_collection = {}\n",
    "\n",
    "# Collecting data for each participant\n",
    "for index, row in df.iterrows():\n",
    "    study = row[\"Study Name\"]\n",
    "    participant = row[\"Respondent Name\"]\n",
    "    saccade_amp = row[\"Saccade Amplitude\"]\n",
    "    saccade_peak_vel = row[\"Saccade Peak Velocity\"]\n",
    "    \n",
    "    #Fix naming convention\n",
    "    df.loc[index, \"Study Name\"] = \"Bridge 1\"\n",
    "\n",
    "    if participant not in participants_collection:\n",
    "        participants_collection[participant] = {\n",
    "            'Saccade Amplitude': [],\n",
    "            'Saccade Peak Velocity': []\n",
    "        }\n",
    "    \n",
    "    participants_collection[participant][\"Saccade Amplitude\"].append(saccade_amp)\n",
    "    participants_collection[participant][\"Saccade Peak Velocity\"].append(abs(saccade_peak_vel))\n",
    "\n",
    "# Calculate mean and std for each participant\n",
    "participants_stats = {}\n",
    "\n",
    "def get_stats(data):\n",
    "    cleaned_data = [float(x) for x in data if x != \"NA\"]\n",
    "    mean_value = np.mean(cleaned_data)\n",
    "    std_dev = np.std(cleaned_data)\n",
    "    return [mean_value, std_dev]\n",
    "\n",
    "for participant, data in participants_collection.items():\n",
    "    participants_stats[participant] = {\n",
    "        \"Saccade Amplitude\": get_stats(data[\"Saccade Amplitude\"]),\n",
    "        \"Saccade Peak Velocity\": get_stats(data[\"Saccade Peak Velocity\"])\n",
    "    }\n",
    "print(participants_collection[20022])\n",
    "print(participants_stats[20022])\n",
    "    \n",
    "    #Now, filter outliers.\n",
    "def check_range(stats, data):\n",
    "    if data == \"NA\":\n",
    "        return False\n",
    "    mean, std = stats\n",
    "    \n",
    "    # Define the range for 3.5 standard deviations\n",
    "    lower_bound = mean - 3.5 * std\n",
    "    upper_bound = mean + 3.5 * std\n",
    "    \n",
    "    return lower_bound <=abs(float(data)) <= upper_bound\n",
    "\n",
    "    \n",
    "filtered_row ={}\n",
    "filtered_row[\"Saccade Amplitude\"] = []\n",
    "filtered_row[\"Saccade Peak Acceleration\"] =[]\n",
    "filtered_row[\"Saccade Peak Deceleration\"] =[]\n",
    "filtered_row[\"Saccade Peak Velocity\"] =[]\n",
    "sacacade_amplitude_count = 0\n",
    "saccade_peak_velocity_count = 0\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    participant= row[\"Respondent Name\"]\n",
    "    saccade_amp = row[\"Saccade Amplitude\"]\n",
    "    saccade_peak_vel = row[\"Saccade Peak Velocity\"]\n",
    "    # saccade_peak_ac = row[\"Saccade Peak Acceleration\"]\n",
    "    # sccade_peak_dec = row[\"Saccade Peak Deceleration\"]\n",
    "    \n",
    "    participant_stat_amp = participants_stats[participant][\"Saccade Amplitude\"]\n",
    "    participant_stat_vel = participants_stats[participant][\"Saccade Peak Velocity\"]\n",
    "    # participant_stat_ac = participants_stats[participant][\"Saccade Peak Acceleration\"]\n",
    "    # participant_stat_dec = participants_stats[participant][\"Saccade Peak Deceleration\"]\n",
    "    if saccade_amp!=\"NA\":\n",
    "        sacacade_amplitude_count +=1\n",
    "    if  saccade_peak_vel!=\"NA\":\n",
    "        saccade_peak_velocity_count +=1\n",
    "    if saccade_amp !=\"NA\" and not check_range(participant_stat_amp, saccade_amp):\n",
    "        df.loc[index, \"Saccade Amplitude\"] = \"NA\"\n",
    "        filtered_row['Saccade Amplitude'].append((index, participant))\n",
    "    if saccade_peak_vel !=\"NA\" and not check_range(participant_stat_vel, saccade_peak_vel):\n",
    "        df.loc[index, \"Saccade peak Velocity\"] = \"NA\"\n",
    "        filtered_row[\"Saccade Peak Velocity\"].append((index, participant))\n",
    "    # if saccade_amp !=\"NA\" and not check_range(participant_stat_ac, saccade_peak_ac):\n",
    "    #     row[\"Saccade Peak Acceleration\"] = \"NA\"\n",
    "    #     filtered_row[\"Saccade Peak Acceleration\"].append(index)\n",
    "    # if saccade_amp !=\"NA\" and not check_range(participant_stat_dec, sccade_peak_dec):\n",
    "    #     row[\"Saccade Peak Deceleration\"] = \"NA\"\n",
    "    #     filtered_row[\"Saccade Peak Deceleration\"].append(index)\n",
    "\n",
    "# Append processed rows to the combined_rows list\n",
    "combined_rows.append(df)\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "combined_df = pd.concat(combined_rows, ignore_index=True)\n",
    "\n",
    "# Sort by \"Study Name\" first in the specified order, then by \"Respondent\" numerically\n",
    "study_order = [\"Bridge 1\", \"Bridge 2\", \"Bridge 3\", \"Bridge 4\"]\n",
    "combined_df['Study Name'] = pd.Categorical(combined_df['Study Name'], categories=study_order, ordered=True)\n",
    "combined_df = combined_df.sort_values(by=['Study Name', 'Respondent Name'])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Crack 1 Hit' 'Crack 2 Hit' 'Crack 3 Hit' 'Crack 4 Hit' 'Crack 5 Hit'\n",
      " 'Base 1' 'Crack 12 Hit' 'Base 2' 'Crack 11 Hit' 'Crack 9 Hit'\n",
      " 'Crack 8 Miss' 'Crack 7 Miss' 'Crack 6 HIt' 'Base 4' 'Crack 15 Hit'\n",
      " 'Crack 13 Hit' 'Crack 16 HIt' 'Base 5' 'Crack 18 Hit' 'Crack 20 Hit'\n",
      " 'Crack 19 Hit' 'Crack 14 Hit' 'Base 3' 'Crack 17 Hit' 'Crack 7 Hit'\n",
      " 'Crack 6 Miss' 'Crack 8 Hit' 'Crack 10 Miss' 'Crack 16 Miss'\n",
      " 'Crack 20 Miss' 'Crack 16 Hit' 'Crack 6 Hit' 'Crack 10 Hit'\n",
      " 'Crack 14 Miss' 'Crack 17 Miss' 'Crack 18 Miss' 'FA 1' 'Crack 5 Miss'\n",
      " 'Crack 12 Miss' 'Crack 15 Miss' 'Crack 8 hit' 'Crack 9 Miss'\n",
      " 'Crack 19 Miss' 'Crack 4 Miss' 'Crack 1 Miss' 'Crack 2 Miss'\n",
      " 'Crack 11 Miss' 'Crack 3 Miss' 'Crack 2 HIt' 'Crack 13 Miss']\n",
      "54954\n"
     ]
    }
   ],
   "source": [
    "# List unique values under the \"AOI Label\" column\n",
    "unique_aoi_labels = combined_df[\"AOI Label\"].unique()\n",
    "\n",
    "# Print the unique values\n",
    "print(unique_aoi_labels)\n",
    "\n",
    "print(len(combined_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Saccade Amplitude': [(1087, 20017), (1091, 20017), (21184, 20021), (21185, 20021), (28502, 20023), (34048, 20024), (37032, 20024), (37045, 20024), (37336, 20024), (37601, 20024), (63111, 20029), (63912, 20029), (67471, 20030), (67476, 20030), (67478, 20030), (69270, 20030), (69611, 20030), (72000, 20031), (73589, 20031), (77185, 20032), (77504, 20032), (77511, 20032), (77669, 20032), (77670, 20032), (78391, 20032), (86054, 20034), (86914, 20034), (87026, 20034), (88100, 20034), (92007, 20034), (94731, 20034), (94761, 20034), (94764, 20034), (95547, 20034), (99172, 20035), (99848, 20035), (101120, 20035), (103152, 20036), (111563, 20037), (111572, 20037), (111573, 20037), (117553, 20038), (120297, 20038), (120334, 20038), (138026, 20042), (138995, 20042), (140592, 20043), (140949, 20043), (141391, 20043), (144321, 20044), (144832, 20044), (146387, 20044), (154192, 20046), (154807, 20046), (155538, 20046), (157156, 20046)], 'Saccade Peak Acceleration': [], 'Saccade Peak Deceleration': [], 'Saccade Peak Velocity': [(1091, 20017), (1258, 20017), (1679, 20017), (18770, 20021), (21184, 20021), (21185, 20021), (28502, 20023), (32986, 20024), (32993, 20024), (34048, 20024), (36702, 20024), (37035, 20024), (37336, 20024), (37601, 20024), (43945, 20026), (43951, 20026), (44087, 20026), (44413, 20026), (44421, 20026), (45281, 20026), (45799, 20026), (47648, 20026), (60291, 20029), (62290, 20029), (65863, 20030), (66999, 20030), (67471, 20030), (67473, 20030), (70811, 20030), (72000, 20031), (73602, 20031), (77189, 20032), (77533, 20032), (77669, 20032), (77670, 20032), (78024, 20032), (78391, 20032), (78407, 20032), (80405, 20032), (88100, 20034), (93329, 20034), (95521, 20034), (95526, 20034), (95527, 20034), (96821, 20034), (99172, 20035), (99304, 20035), (99340, 20035), (99848, 20035), (101120, 20035), (101736, 20035), (106624, 20036), (119064, 20038), (119839, 20038), (119938, 20038), (122460, 20039), (140703, 20043), (140809, 20043), (141391, 20043), (141508, 20043), (142375, 20043), (142475, 20043), (142604, 20043), (143845, 20044), (148060, 20044), (148072, 20044), (148076, 20044), (148079, 20044), (154807, 20046), (155560, 20046), (157156, 20046), (157389, 20046), (157812, 20046), (158292, 20046)]}\n",
      "percentage of saccade ampitude filtered:  0.010911925175370226\n",
      "percentage of peak velocity filtered:  0.008780256288561937\n"
     ]
    }
   ],
   "source": [
    "print(filtered_row)\n",
    "print(\"percentage of saccade ampitude filtered: \", len(filtered_row['Saccade Amplitude'])/sacacade_amplitude_count)\n",
    "print(\"percentage of peak velocity filtered: \", len(filtered_row['Saccade Peak Velocity'])/saccade_peak_velocity_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Study Name', 'Respondent', 'Start', 'End', 'Label'], dtype='object')\n",
      "Index(['Study Name', 'Respondent Name', 'Gaze Calibration', 'Stimulus Start',\n",
      "       'Stimulus Duration', 'Saccade Index', 'Saccade Index by Stimulus',\n",
      "       'Saccade Start', 'Saccade End', 'Saccade Duration', 'Saccade Amplitude',\n",
      "       'Saccade Peak Velocity', 'Saccade Peak Acceleration',\n",
      "       'Saccade Peak Deceleration', 'Saccade Direction', 'Saccade Type',\n",
      "       'AOI Intersections', 'AOI Label', 'AOI Type', 'AOI Group',\n",
      "       'AOI Instance', 'AOI Instance Start', 'AOI Instance Duration',\n",
      "       'Saccade Index by AOI', 'Dwell Index (on AOI)', 'Saccade peak Velocity',\n",
      "       'weight'],\n",
      "      dtype='object')\n",
      "Bridge 1:  [20002 20004 20005 20008 20011 20012 20013 20014 20015 20016 20017 20019\n",
      " 20021 20024 20025 20026 20027 20029 20030 20031 20032 20034 20035 20036\n",
      " 20037 20038 20039 20041 20042 20043 20044 20045 20046]\n",
      "Bridge 2:  [20002 20004 20005 20006 20008 20009 20010 20012 20013 20015 20016 20017\n",
      " 20021 20023 20024 20026 20029 20030 20031 20032 20034 20035 20036 20037\n",
      " 20038 20039 20042 20043 20044 20046]\n",
      "Bridge 3:  [20002 20003 20004 20005 20006 20007 20008 20009 20012 20015 20016 20023\n",
      " 20024 20026 20027 20030 20032 20034 20035 20036 20037 20038 20039 20041\n",
      " 20042 20043 20044 20045 20046 20017 20019 20020 20022]\n",
      "Bridge 4:  [20002 20004 20005 20006 20009 20010 20011 20015 20016 20017 20019 20021\n",
      " 20023 20024 20025 20026 20027 20029 20030 20031 20032 20034 20035 20036\n",
      " 20037 20038 20039 20042 20043 20044 20045 20046]\n"
     ]
    }
   ],
   "source": [
    "#Read timecard.csv\n",
    "combined_timecard_df= pd.read_csv('combined_timecard.csv', skiprows=0)\n",
    "print(combined_timecard_df.columns)\n",
    "\n",
    "combined_df= pd.read_csv('combined_filtered_saccade_table.csv', skiprows=0)\n",
    "combined_df['weight'] = 1\n",
    "print(combined_df.columns)\n",
    "\n",
    "combined_df = pd.read_csv('combined_filtered_saccade_table.csv')\n",
    "unique_respondents_bridge_1 = combined_df[combined_df[\"Study Name\"] == \"Bridge 1\"][\"Respondent Name\"].unique()\n",
    "\n",
    "print('Bridge 1: ', unique_respondents_bridge_1)\n",
    "\n",
    "unique_respondents_bridge_2 = combined_df[combined_df[\"Study Name\"] == \"Bridge 2\"][\"Respondent Name\"].unique()\n",
    "\n",
    "print('Bridge 2: ', unique_respondents_bridge_2)\n",
    "\n",
    "unique_respondents_bridge_3 = combined_df[combined_df[\"Study Name\"] == \"Bridge 3\"][\"Respondent Name\"].unique()\n",
    "\n",
    "print('Bridge 3: ', unique_respondents_bridge_3)\n",
    "unique_respondents_bridge_4 = combined_df[combined_df[\"Study Name\"] == \"Bridge 4\"][\"Respondent Name\"].unique()\n",
    "\n",
    "print('Bridge 4: ', unique_respondents_bridge_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Standardize columns by stripping whitespace and converting to lowercase for consistency\n",
    "# combined_df[\"Study Name\"] = combined_df[\"Study Name\"].str.strip().str.lower()\n",
    "# combined_df[\"Respondent Name\"] = combined_df[\"Respondent Name\"].astype(str).str.strip()\n",
    "# combined_df[\"AOI Label\"] = combined_df[\"AOI Label\"].str.strip().str.lower()\n",
    "\n",
    "# combined_timecard_df[\"Study Name\"] = combined_timecard_df[\"Study Name\"].str.strip().str.lower()\n",
    "# combined_timecard_df[\"Respondent\"] = combined_timecard_df[\"Respondent\"].astype(str).str.strip()\n",
    "# combined_timecard_df[\"Label\"] = combined_timecard_df[\"Label\"].str.strip().str.lower()\n",
    "\n",
    "rows_to_drop =[]\n",
    "half_weight=0\n",
    "# Iterate through each row in the DataFrame\n",
    "for index, row in combined_df.iterrows():\n",
    "    study = row[\"Study Name\"]\n",
    "    participant = row[\"Respondent Name\"]\n",
    "    aoiLabel = row[\"AOI Label\"]\n",
    "    saccade_start = row[\"Saccade Start\"]\n",
    "    saccade_end = row[\"Saccade End\"]\n",
    "    \n",
    "    time_row = combined_timecard_df[(combined_timecard_df[\"Respondent\"]==participant) & (combined_timecard_df[\"Label\"]==aoiLabel) & (combined_timecard_df[\"Study Name\"]==study)] #There should be a unique match\n",
    "    \n",
    "    if time_row.empty:\n",
    "        print(\"Row empty: \", participant, aoiLabel, study) #This shouldn't happen\n",
    "    if time_row.shape[0] > 1:\n",
    "        print(\"Multiple rows: \", participant, aoiLabel, study) #This shouldn't happen\n",
    "        print(time_row)\n",
    "    \n",
    "    start = time_row[\"Start\"].values[0]\n",
    "    end = time_row[\"End\"].values[0]\n",
    "    \n",
    "    #drop the rows if saccade is outside the range completely\n",
    "    if saccade_end < start or saccade_start > end:\n",
    "        rows_to_drop.append(index)\n",
    "    \n",
    "    #saccade is partially covered by time window\n",
    "    if saccade_start < start or saccade_end > end: \n",
    "        half_weight +=1\n",
    "        combined_df.loc[index, 'weight'] = 0.5\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24471\n",
      "24926\n"
     ]
    }
   ],
   "source": [
    "print(len(rows_to_drop))\n",
    "print(half_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows where weight is 0.5: 455\n"
     ]
    }
   ],
   "source": [
    "# Drop the rows from the original DataFrame\n",
    "df_filtered = combined_df.drop(rows_to_drop)\n",
    "\n",
    "# Count the number of rows where 'weight' equals 0.5\n",
    "count = df_filtered[df_filtered['weight'] == 0.5].shape[0]\n",
    "\n",
    "# Print the result\n",
    "print(f'Number of rows where weight is 0.5: {count}')\n",
    "\n",
    "# Save the modified DataFrame to a new CSV file (optional)\n",
    "df_filtered.to_csv('combined_filtered_SaccadeTable.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Study Name  Respondent Name Gaze Calibration  Stimulus Start  \\\n",
      "0   Bridge 1            20002        Excellent       1534.4453   \n",
      "1   Bridge 1            20002        Excellent       1534.4453   \n",
      "2   Bridge 1            20002        Excellent       1534.4453   \n",
      "3   Bridge 1            20002        Excellent       1534.4453   \n",
      "4   Bridge 1            20002        Excellent       1534.4453   \n",
      "\n",
      "   Stimulus Duration  Saccade Index  Saccade Index by Stimulus  Saccade Start  \\\n",
      "0          683570.31            474                        468     106287.310   \n",
      "1          683570.31            475                        469     106487.371   \n",
      "2          683570.31            479                        473     106845.642   \n",
      "3          683570.31            480                        474     106970.664   \n",
      "4          683570.31            481                        475     106995.645   \n",
      "\n",
      "   Saccade End  Saccade Duration  ...    AOI Label     AOI Type  AOI Group  \\\n",
      "0   106320.635           33.3256  ...  Crack 1 Hit  Dynamic AOI       Hits   \n",
      "1   106520.663           33.2920  ...  Crack 1 Hit  Dynamic AOI       Hits   \n",
      "2   106903.974           58.3320  ...  Crack 1 Hit  Dynamic AOI       Hits   \n",
      "3   106979.010            8.3464  ...  Crack 1 Hit  Dynamic AOI       Hits   \n",
      "4   107004.001            8.3568  ...  Crack 1 Hit  Dynamic AOI       Hits   \n",
      "\n",
      "   AOI Instance  AOI Instance Start AOI Instance Duration  \\\n",
      "0             1           104936.07                4800.0   \n",
      "1             1           104936.07                4800.0   \n",
      "2             1           104936.07                4800.0   \n",
      "3             1           104936.07                4800.0   \n",
      "4             1           104936.07                4800.0   \n",
      "\n",
      "   Saccade Index by AOI Dwell Index (on AOI) Saccade peak Velocity weight  \n",
      "0                     3                  1.0                   NaN    1.0  \n",
      "1                     4                  NaN                   NaN    1.0  \n",
      "2                     5                  NaN                   NaN    1.0  \n",
      "3                     6                  2.0                   NaN    1.0  \n",
      "4                     7                  2.0                   NaN    1.0  \n",
      "\n",
      "[5 rows x 27 columns]\n"
     ]
    }
   ],
   "source": [
    "processed_combined_df = pd.read_csv('combined_filtered_SaccadeTable.csv')\n",
    "print(processed_combined_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Study Name', 'Respondent Name', 'Gaze Calibration', 'Stimulus Start',\n",
      "       'Stimulus Duration', 'Saccade Index', 'Saccade Index by Stimulus',\n",
      "       'Saccade Start', 'Saccade End', 'Saccade Duration', 'Saccade Amplitude',\n",
      "       'Saccade Peak Velocity', 'Saccade Peak Acceleration',\n",
      "       'Saccade Peak Deceleration', 'Saccade Direction', 'Saccade Type',\n",
      "       'AOI Intersections', 'AOI Label', 'AOI Type', 'AOI Group',\n",
      "       'AOI Instance', 'AOI Instance Start', 'AOI Instance Duration',\n",
      "       'Saccade Index by AOI', 'Dwell Index (on AOI)', 'Saccade peak Velocity',\n",
      "       'weight'],\n",
      "      dtype='object')\n",
      "[20002 20004 20005 20008 20011 20012 20013 20014 20015 20016 20017 20019\n",
      " 20021 20024 20025 20026 20027 20029 20030 20031 20032 20034 20035 20036\n",
      " 20037 20038 20039 20041 20042 20043 20044 20045 20046 20006 20009 20010\n",
      " 20023 20003 20007 20020 20022]\n",
      "['Hits' 'Misses' 'Base' 'Hit' 'Miss' 'Bases' nan]\n",
      "[1 3 2]\n",
      "30483\n"
     ]
    }
   ],
   "source": [
    "#Inspect\n",
    "print(processed_combined_df.columns)\n",
    "# List unique values under the \"Respondent Name\" column\n",
    "unique_respondents = processed_combined_df[\"Respondent Name\"].unique()\n",
    "\n",
    "# Print the unique values\n",
    "print(unique_respondents)\n",
    "\n",
    "# List unique values under the \"AOI Group\" column\n",
    "unique_aoi_groups = processed_combined_df[\"AOI Group\"].unique()\n",
    "\n",
    "# Print the unique values\n",
    "print(unique_aoi_groups)\n",
    "\n",
    "# List unique values under the \"AOI Instance\" column\n",
    "unique_aoi_instance_start = processed_combined_df[\"AOI Instance\"].unique()\n",
    "\n",
    "# Print the unique values\n",
    "print(unique_aoi_instance_start)\n",
    "\n",
    "print(len(processed_combined_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next,we process the filtered table.\n",
    "#Drop Gaze Calibration, Stimulus Start, Stimulus Duration, Saccade Index, Saccade Index by Stimulus, Saccade Type, Saccade Index by AOI, Dwell Index (on AOI), AOI Instance Duration, AOI Instance Start, AOI Type, AOI Intersections\n",
    "#Drop Saccade Direction\n",
    "#Saccade duration, amplitude, peak velocity, peak acceleration, peak deceleration -> mean, median, standard deviation\n",
    "#Create a new column counting a number of saccade\n",
    "\n",
    "#Drop columns\n",
    "columns_to_drop = [\"Gaze Calibration\", \"Stimulus Start\", \"Stimulus Duration\", \"Saccade Start\", \"Saccade End\", \"Saccade Index\", \"Saccade Index by Stimulus\", \"Saccade Type\", \"Saccade Index by AOI\", \"Dwell Index (on AOI)\",\n",
    "                   \"AOI Instance Duration\", \"AOI Instance Start\", \"AOI Type\", \"AOI Intersections\", \"Saccade Direction\", \"AOI Instance\"]\n",
    "\n",
    "# Drop columns only if they exist in the DataFrame\n",
    "processed_df = processed_combined_df.drop([col for col in columns_to_drop if col in df.columns], axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the aggrevated stats\n",
    "# columns_to_aggrevate = [\"Saccade Duration\", \"Saccade Amplitude\", \"Saccade Peak Velocity\", \"Saccade Peak Acceleration\", 'Saccade Peak Deceleration']\n",
    "# aggrevate_dict = {col: [\"mean\", \"median\", \"std\"] for col in columns_to_aggrevate}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN Percentage: 40.14%\n"
     ]
    }
   ],
   "source": [
    "row_count = 0\n",
    "amplitude_count = 0\n",
    "\n",
    "for index, row in processed_combined_df.iterrows():\n",
    "    row_count += 1\n",
    "    if pd.isna(row[\"Saccade Amplitude\"]):  # Check if the value is NaN\n",
    "        amplitude_count += 1\n",
    "\n",
    "print(f\"NaN Percentage: {amplitude_count / row_count * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining custom functions to calculate weighted_mean and weighted_std\n",
    "def weighted_mean(values, weights):\n",
    "    # Remove NaN values from both values and weights\n",
    "    mask = ~pd.isna(values) & ~pd.isna(weights)\n",
    "    valid_values = values[mask].astype(float)\n",
    "    valid_weights = weights[mask].astype(float)\n",
    "    \n",
    "    if len(valid_values) > 0:\n",
    "        return np.average(valid_values, weights=valid_weights)\n",
    "    else:\n",
    "        return np.nan  # Return NaN if no valid data remains\n",
    "\n",
    "def weighted_std(values, weights):\n",
    "    # Remove NaN values from both values and weights\n",
    "    mask = ~pd.isna(values) & ~pd.isna(weights)\n",
    "    valid_values = values[mask].astype(float)\n",
    "    valid_weights = weights[mask].astype(float)\n",
    "    \n",
    "    if len(valid_values) > 0:\n",
    "        average = weighted_mean(valid_values, valid_weights)\n",
    "        variance = np.average((valid_values - average) ** 2, weights=valid_weights)\n",
    "        return np.sqrt(variance)\n",
    "    else:\n",
    "        return np.nan  # Return NaN if no valid data remains\n",
    "\n",
    "def weighted_count(values):\n",
    "    return np.sum(values)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I need clarification on what NA means for those columns above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate statistics for each group (AOI label + participant)\n",
    "def custom_agg(group):\n",
    "    return pd.Series({\n",
    "        'Saccade Duration Mean': weighted_mean(group['Saccade Duration'], group['weight']),\n",
    "        'Saccade Duration Std': weighted_std(group['Saccade Duration'], group['weight']),\n",
    "        'Saccade Duration Median': group['Saccade Duration'].median(),\n",
    "        'Saccade Amplitude Mean': weighted_mean(group['Saccade Amplitude'], group['weight']),\n",
    "        'Saccade Amplitude Std': weighted_std(group['Saccade Amplitude'], group['weight']),\n",
    "        'Saccade Amplitude Median': group['Saccade Amplitude'].median(),\n",
    "        'Saccade Peak Velocity Mean': weighted_mean(group['Saccade Peak Velocity'], group['weight']),\n",
    "        'Saccade Peak Velocity Std': weighted_std(group['Saccade Peak Velocity'], group['weight']),\n",
    "        'Saccade Peak Velocity Median': group['Saccade Peak Velocity'].median(),\n",
    "        'Saccade Peak Acceleration Mean': weighted_mean(group['Saccade Peak Acceleration'], group['weight']),\n",
    "        'Saccade Peak Acceleration Std': weighted_std(group['Saccade Peak Acceleration'], group['weight']),\n",
    "        'Saccade Peak Acceleration Median': group['Saccade Peak Acceleration'].median(),\n",
    "        'Saccade Peak Deceleration Mean': weighted_mean(group['Saccade Peak Deceleration'], group['weight']),\n",
    "        'Saccade Peak Deceleration Std': weighted_std(group['Saccade Peak Deceleration'], group['weight']),\n",
    "        'Saccade Peak Deceleration Median': group['Saccade Peak Deceleration'].median(),\n",
    "        \"Saccade Count\" : weighted_count(group['weight']),\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the modified DataFrame to a new CSV file (optional)\n",
    "grouped_processed_combined_df = processed_combined_df.groupby([\"Respondent Name\", \"AOI Label\", \"Study Name\"]).apply(custom_agg).reset_index()\n",
    "\n",
    "# # Flatten the multi-level column names\n",
    "# grouped_processed_df.columns = [' '.join(col).strip() for col in grouped_processed_df.columns.values]\n",
    "\n",
    "grouped_processed_combined_df['Study Name'] = pd.Categorical(grouped_processed_combined_df['Study Name'], categories=study_order, ordered=True)\n",
    "\n",
    "grouped_processed_combined_df.to_csv('combined_processed_SaccadeTable.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bridge 1:  [20002 20004 20005 20008 20011 20012 20013 20014 20015 20016 20017 20019\n",
      " 20021 20024 20025 20026 20027 20029 20030 20031 20032 20034 20035 20036\n",
      " 20037 20038 20039 20041 20042 20043 20044 20045 20046]\n",
      "Bridge 2:  [20002 20004 20005 20006 20008 20009 20010 20012 20013 20015 20016 20017\n",
      " 20021 20023 20024 20026 20029 20030 20031 20032 20034 20035 20036 20037\n",
      " 20038 20039 20042 20043 20044 20046]\n",
      "Bridge 3:  [20002 20003 20004 20005 20006 20007 20008 20009 20012 20015 20016 20017\n",
      " 20019 20020 20022 20023 20024 20026 20027 20030 20032 20034 20035 20036\n",
      " 20037 20038 20039 20041 20042 20043 20044 20045 20046]\n",
      "Bridge 4:  [20002 20004 20005 20006 20009 20010 20011 20015 20016 20017 20019 20021\n",
      " 20023 20024 20025 20026 20027 20029 20030 20031 20032 20034 20035 20036\n",
      " 20037 20038 20039 20042 20043 20044 20045 20046]\n",
      "[20002, 20004, 20005, 20008, 20011, 20012, 20013, 20014, 20015, 20016, 20017, 20019, 20021, 20024, 20025, 20026, 20027, 20029, 20030, 20031, 20032, 20034, 20035, 20036, 20037, 20038, 20039, 20041, 20042, 20043, 20044, 20045, 20046]\n",
      "[20002, 20004, 20005, 20006, 20008, 20009, 20010, 20012, 20013, 20015, 20016, 20017, 20021, 20023, 20024, 20026, 20029, 20030, 20031, 20032, 20034, 20035, 20036, 20037, 20038, 20039, 20042, 20043, 20044, 20046]\n",
      "[20002, 20003, 20004, 20005, 20006, 20007, 20008, 20009, 20012, 20015, 20016, 20017, 20019, 20020, 20022, 20023, 20024, 20026, 20027, 20030, 20032, 20034, 20035, 20036, 20037, 20038, 20039, 20041, 20042, 20043, 20044, 20045, 20046]\n",
      "[20002, 20004, 20005, 20006, 20009, 20010, 20011, 20015, 20016, 20017, 20019, 20021, 20023, 20024, 20025, 20026, 20027, 20029, 20030, 20031, 20032, 20034, 20035, 20036, 20037, 20038, 20039, 20042, 20043, 20044, 20045, 20046]\n"
     ]
    }
   ],
   "source": [
    "#Verify\n",
    "processed_combined_df = pd.read_csv('combined_processed_SaccadeTable.csv')\n",
    "unique_respondents_bridge_1 = processed_combined_df[processed_combined_df[\"Study Name\"] == \"Bridge 1\"][\"Respondent Name\"].unique()\n",
    "\n",
    "print('Bridge 1: ', unique_respondents_bridge_1)\n",
    "\n",
    "unique_respondents_bridge_2 = processed_combined_df[processed_combined_df[\"Study Name\"] == \"Bridge 2\"][\"Respondent Name\"].unique()\n",
    "\n",
    "print('Bridge 2: ', unique_respondents_bridge_2)\n",
    "\n",
    "unique_respondents_bridge_3 = processed_combined_df[processed_combined_df[\"Study Name\"] == \"Bridge 3\"][\"Respondent Name\"].unique()\n",
    "\n",
    "print('Bridge 3: ', unique_respondents_bridge_3)\n",
    "unique_respondents_bridge_4 = processed_combined_df[processed_combined_df[\"Study Name\"] == \"Bridge 4\"][\"Respondent Name\"].unique()\n",
    "\n",
    "print('Bridge 4: ', unique_respondents_bridge_4)\n",
    "\n",
    "# participant1 = list(unique_respondents_bridge_1)\n",
    "# participant2 = list(unique_respondents_bridge_2)\n",
    "# participant3 = list(unique_respondents_bridge_3)\n",
    "# participant4 = list(unique_respondents_bridge_4)\n",
    "# print(participant1)\n",
    "# print(participant2)\n",
    "# print(participant3)\n",
    "# print(participant4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15 (main, Oct  3 2024, 02:33:33) [Clang 14.0.6 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2c229cd6ff87d19ea7d47541bfa1b62a250f965db58eb2430f02bd66bc83489c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
