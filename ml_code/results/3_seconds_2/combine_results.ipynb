{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from functools import reduce\n",
    "import glob \n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary saved to metrics_summary.csv\n"
     ]
    }
   ],
   "source": [
    "# Paths for each model\n",
    "svm_folder_path = \"svm/\"  \n",
    "rf_folder_path = \"rf/\"\n",
    "cnn_folder_path = \"cnn/\"\n",
    "paths = [svm_folder_path, rf_folder_path, cnn_folder_path]\n",
    "\n",
    "summary_rows = []\n",
    "\n",
    "for folder_path in paths:\n",
    "    # Identify which model folder we're in\n",
    "    if folder_path.startswith(\"svm\"):\n",
    "        model = \"svm\"\n",
    "    elif folder_path.startswith(\"rf\"):\n",
    "        model = \"rf\"\n",
    "    elif folder_path.startswith(\"cnn\"):\n",
    "        model = \"cnn\"\n",
    "\n",
    "    # Scan folder for CSV metrics\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(f\"_{model}_metrics.csv\"):\n",
    "            match = re.match(fr\"(.+?)_(original|oversampled)_{model}_metrics\\.csv\", filename)\n",
    "            if match:\n",
    "                data_name, sampling_type = match.groups()\n",
    "                filepath = os.path.join(folder_path, filename)\n",
    "\n",
    "                df = pd.read_csv(filepath)\n",
    "                avg_mcc = df[\"MCC\"].mean()\n",
    "                avg_f1_macro = df[\"F1_macro\"].mean()\n",
    "                avg_f1_weighted = df[\"F1_weighted\"].mean()\n",
    "\n",
    "                summary_rows.append({\n",
    "                    \"Model\": model,\n",
    "                    \"Data\": data_name,\n",
    "                    \"Sampling\": sampling_type,\n",
    "                    \"Avg_MCC\": avg_mcc,\n",
    "                    \"Avg_F1_macro\": avg_f1_macro,\n",
    "                    \"Avg_F1_weighted\": avg_f1_weighted\n",
    "                })\n",
    "\n",
    "# Convert to DataFrame\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "\n",
    "# Pivot so that \"original\" and \"oversampled\" become separate columns\n",
    "summary_wide = summary_df.pivot(\n",
    "    index=[\"Data\", \"Model\"], \n",
    "    columns=\"Sampling\", \n",
    "    values=[\"Avg_MCC\", \"Avg_F1_macro\", \"Avg_F1_weighted\"]\n",
    ")\n",
    "\n",
    "# Flatten multi-level columns: (\"Avg_MCC\", \"original\") -> \"Avg_MCC_original\"\n",
    "summary_wide.columns = [f\"{col[0]}_{col[1]}\" for col in summary_wide.columns]\n",
    "\n",
    "# Bring \"Data\" and \"Model\" back as columns\n",
    "summary_wide.reset_index(inplace=True)\n",
    "\n",
    "# Rename columns to your desired format\n",
    "summary_wide.rename(columns={\n",
    "    \"Data\": \"data type\",\n",
    "    \"Model\": \"model\",\n",
    "    \"Avg_MCC_original\": \"MCC Original\",\n",
    "    \"Avg_MCC_oversampled\": \"MCC Oversampled\",\n",
    "    \"Avg_F1_macro_original\": \"macro f1-score Original\",\n",
    "    \"Avg_F1_macro_oversampled\": \"macro f1-score Oversampled\",\n",
    "    \"Avg_F1_weighted_original\": \"weighted f1-score Original\",\n",
    "    \"Avg_F1_weighted_oversampled\": \"weighted f1-score Oversampled\",\n",
    "}, inplace=True)\n",
    "\n",
    "# Reorder columns to the exact sequence you want\n",
    "summary_wide = summary_wide[\n",
    "    [\n",
    "        \"data type\",\n",
    "        \"model\",\n",
    "        \"MCC Original\",\n",
    "        \"MCC Oversampled\",\n",
    "        \"weighted f1-score Original\",\n",
    "        \"weighted f1-score Oversampled\",\n",
    "        \"macro f1-score Original\",\n",
    "        \"macro f1-score Oversampled\"\n",
    "    ]\n",
    "]\n",
    "\n",
    "# Finally, save to CSV in the new wide format\n",
    "summary_wide.to_csv(\"metrics_summary.csv\", index=False)\n",
    "print(\"Summary saved to metrics_summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to t_test_results_all_in_one_row.csv!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from scipy.stats import ttest_ind\n",
    "import os\n",
    "\n",
    "# Base setup\n",
    "models = [\"svm\", \"rf\", \"cnn\"]\n",
    "metrics = [\"MCC\", \"F1_macro\", \"F1_weighted\"]\n",
    "results = []\n",
    "\n",
    "for model in models:\n",
    "    base_dir = f\"{model}/\"\n",
    "    \n",
    "    # Load full benchmark datasets\n",
    "    full_original = pd.read_csv(f\"{base_dir}all_original_{model}_metrics.csv\")\n",
    "    full_oversampled = pd.read_csv(f\"{base_dir}all_oversampled_{model}_metrics.csv\")\n",
    "\n",
    "    # Loop through each ablation file in the folder\n",
    "    for filename in os.listdir(base_dir):\n",
    "        match = re.match(fr\"(.+?)_(original|oversampled)_{model}_metrics\\.csv\", filename)\n",
    "        if match and not filename.startswith(\"all_\"):  # Skip the 'all_original'/'all_oversampled' full files\n",
    "            data_name, sampling_type = match.groups()\n",
    "            ablation_path = os.path.join(base_dir, filename)\n",
    "            ablation_df = pd.read_csv(ablation_path)\n",
    "\n",
    "            # Select baseline (original or oversampled)\n",
    "            full_df = full_original if sampling_type == \"original\" else full_oversampled\n",
    "\n",
    "            # Run t-tests for each metric\n",
    "            for metric in metrics:\n",
    "                t_stat, p_val = ttest_ind(full_df[metric], ablation_df[metric], equal_var=False)\n",
    "                results.append({\n",
    "                    \"Data\": data_name,\n",
    "                    \"Model\": model,\n",
    "                    \"Metric\": metric,\n",
    "                    \"Sampling\": sampling_type,   # \"original\" or \"oversampled\"\n",
    "                    \"T-stat\": round(t_stat, 4),\n",
    "                    \"P-value\": round(p_val, 4)\n",
    "                })\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# We want a single row for each (Data, Model).\n",
    "# We'll pivot on [Metric, Sampling] for columns, and get T-stat/P-value as the values.\n",
    "t_test_pivot = results_df.pivot_table(\n",
    "    index=[\"Data\", \"Model\"],\n",
    "    columns=[\"Metric\", \"Sampling\"],\n",
    "    values=[\"T-stat\", \"P-value\"]\n",
    ")\n",
    "\n",
    "# t_test_pivot will have multi-level columns like:\n",
    "#   (\"T-stat\", \"MCC\", \"original\"), (\"T-stat\", \"MCC\", \"oversampled\"), \n",
    "#   (\"P-value\", \"MCC\", \"original\"), etc.\n",
    "\n",
    "# Flatten the multi-level columns. \n",
    "# E.g. (\"T-stat\", \"MCC\", \"original\") -> \"MCC Original T-stat\"\n",
    "flattened_cols = []\n",
    "for (stat_type, metric, sampling) in t_test_pivot.columns:\n",
    "    # stat_type is \"T-stat\" or \"P-value\"\n",
    "    # metric is \"MCC\", \"F1_macro\", or \"F1_weighted\"\n",
    "    # sampling is \"original\" or \"oversampled\"\n",
    "    new_col_name = f\"{metric} {sampling.capitalize()} {stat_type}\"\n",
    "    flattened_cols.append(new_col_name)\n",
    "\n",
    "t_test_pivot.columns = flattened_cols\n",
    "\n",
    "# Bring \"Data\" and \"Model\" back as columns\n",
    "t_test_pivot.reset_index(inplace=True)\n",
    "\n",
    "# OPTIONAL: If you want a specific column order, create a list:\n",
    "# E.g., we can manually specify a nice sequence:\n",
    "desired_order = [\n",
    "    \"Data\", \n",
    "    \"Model\",\n",
    "    \"MCC Original T-stat\", \"MCC Original P-value\", \n",
    "    \"MCC Oversampled T-stat\", \"MCC Oversampled P-value\",\n",
    "    \"F1_macro Original T-stat\", \"F1_macro Original P-value\",\n",
    "    \"F1_macro Oversampled T-stat\", \"F1_macro Oversampled P-value\",\n",
    "    \"F1_weighted Original T-stat\", \"F1_weighted Original P-value\",\n",
    "    \"F1_weighted Oversampled T-stat\", \"F1_weighted Oversampled P-value\",\n",
    "]\n",
    "# Filter to existing columns just in case any metric is missing for some reason\n",
    "final_columns = [col for col in desired_order if col in t_test_pivot.columns]\n",
    "t_test_pivot = t_test_pivot[final_columns]\n",
    "\n",
    "# Sort by Data first, then by Model\n",
    "t_test_pivot.sort_values(by=[\"Data\", \"Model\"], inplace=True)\n",
    "\n",
    "# Save to CSV\n",
    "t_test_pivot.to_csv(\"t_test_results_all_in_one_row.csv\", index=False)\n",
    "print(\"Saved to t_test_results_all_in_one_row.csv!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# We want to compare SVM vs. RF for each sampling (original / oversampled).\n",
    "metrics = [\"MCC\", \"F1_macro\", \"F1_weighted\"]\n",
    "results = []\n",
    "\n",
    "# 1) Load SVM \"all_*\" CSVs\n",
    "svm_original_path = \"svm/all_original_svm_metrics.csv\"\n",
    "svm_oversampled_path = \"svm/all_oversampled_svm_metrics.csv\"\n",
    "\n",
    "df_svm_original = pd.read_csv(svm_original_path)\n",
    "df_svm_oversampled = pd.read_csv(svm_oversampled_path)\n",
    "\n",
    "# 2) Look for the RF \"all_*\" CSVs and do a t-test against SVM\n",
    "base_dir = \"rf/\"\n",
    "for filename in os.listdir(base_dir):\n",
    "    # We'll look for \"all_original_rf_metrics.csv\" or \"all_oversampled_rf_metrics.csv\"\n",
    "    match = re.match(r\"all_(original|oversampled)_rf_metrics\\.csv\", filename)\n",
    "    if match:\n",
    "        sampling_type = match.group(1)  # \"original\" or \"oversampled\"\n",
    "        \n",
    "        rf_path = os.path.join(base_dir, filename)\n",
    "        df_rf = pd.read_csv(rf_path)\n",
    "\n",
    "        # Decide which SVM baseline to compare against\n",
    "        if sampling_type == \"original\":\n",
    "            df_svm = df_svm_original\n",
    "        else:  # oversampled\n",
    "            df_svm = df_svm_oversampled\n",
    "\n",
    "        # For each metric, do t-test: SVM vs RF\n",
    "        for metric in metrics:\n",
    "            t_stat, p_val = ttest_ind(df_svm[metric], df_rf[metric], equal_var=False)\n",
    "            results.append({\n",
    "                \"Data\": \"all\",             # Because the filename was \"all_...\"\n",
    "                \"Metric\": metric,\n",
    "                \"Model1\": \"svm\",\n",
    "                \"Model2\": \"rf\",\n",
    "                \"Sampling\": sampling_type,\n",
    "                \"T-stat\": round(t_stat, 4),\n",
    "                \"P-value\": round(p_val, 4)\n",
    "            })\n",
    "\n",
    "# Convert all results to a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# ---------- Pivoting to put everything on ONE row per (Data) -----------\n",
    "# We'll pivot on: (Data, Sampling) as the row index, \n",
    "# and (Metric, [T-stat/P-value]) as columns. But we also have Model1 vs. Model2 in there.\n",
    "# You might prefer a simpler pivot that just shows T-stat/P-value in columns.\n",
    "\n",
    "# Here's an approach that puts each (Metric, T-stat, P-value) in columns, \n",
    "# distinguishing them by \"Model1_vs_Model2\" or \"Sampling\" or both.\n",
    "\n",
    "# If your main goal is to have \"MCC Original T-stat\", \"MCC Original P-value\", etc.,\n",
    "# you can do a triple pivot with index=[\"Data\"], columns=[\"Sampling\",\"Metric\"], values=[\"T-stat\",\"P-value\"].\n",
    "# That lumps the model comparison into the row data (since we only have one comparison: SVM vs RF).\n",
    "\n",
    "t_test_pivot = results_df.pivot_table(\n",
    "    index=[\"Data\"],                    # e.g. \"all\"\n",
    "    columns=[\"Sampling\", \"Metric\"],    # (original|oversampled), (MCC|F1_macro|F1_weighted)\n",
    "    values=[\"T-stat\", \"P-value\"]       # We want T-stat and P-value\n",
    ")\n",
    "\n",
    "# This yields multi-level columns like:\n",
    "# (\"T-stat\", \"original\", \"MCC\"), (\"T-stat\", \"original\", \"F1_macro\"), ...\n",
    "# We'll flatten them.\n",
    "\n",
    "flattened_cols = []\n",
    "for (stat_type, sampling, metric) in t_test_pivot.columns:\n",
    "    # Example: stat_type=\"T-stat\", sampling=\"original\", metric=\"MCC\"\n",
    "    # We'll produce \"MCC Original T-stat\".\n",
    "    new_col_name = f\"{metric} {sampling.capitalize()} {stat_type}\"\n",
    "    flattened_cols.append(new_col_name)\n",
    "\n",
    "t_test_pivot.columns = flattened_cols\n",
    "t_test_pivot.reset_index(inplace=True)\n",
    "\n",
    "# If you want a certain column ordering:\n",
    "desired_order = [\n",
    "    \"Data\",\n",
    "    \"MCC Original T-stat\", \"MCC Original P-value\",\n",
    "    \"MCC Oversampled T-stat\", \"MCC Oversampled P-value\",\n",
    "    \"F1_macro Original T-stat\", \"F1_macro Original P-value\",\n",
    "    \"F1_macro Oversampled T-stat\", \"F1_macro Oversampled P-value\",\n",
    "    \"F1_weighted Original T-stat\", \"F1_weighted Original P-value\",\n",
    "    \"F1_weighted Oversampled T-stat\", \"F1_weighted Oversampled P-value\",\n",
    "]\n",
    "# Keep only columns that exist in the pivot\n",
    "final_cols = [c for c in desired_order if c in t_test_pivot.columns]\n",
    "t_test_pivot = t_test_pivot[final_cols]\n",
    "\n",
    "# Save\n",
    "t_test_pivot.to_csv(\"t_test_results_svm_rf.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# 1) Load CNN \"all_original_cnn_metrics.csv\" and \"all_oversampled_cnn_metrics.csv\"\n",
    "cnn_original_path = \"svm/all_original_svm_metrics.csv\"\n",
    "cnn_oversampled_path = \"svm/all_oversampled_svm_metrics.csv\"\n",
    "\n",
    "df_cnn_original = pd.read_csv(cnn_original_path)\n",
    "df_cnn_oversampled = pd.read_csv(cnn_oversampled_path)\n",
    "\n",
    "# 2) Define metrics to test\n",
    "metrics = [\"MCC\", \"F1_macro\", \"F1_weighted\"]\n",
    "\n",
    "# 3) Run t-tests\n",
    "results = []\n",
    "for metric in metrics:\n",
    "    t_stat, p_val = ttest_ind(df_cnn_original[metric], df_cnn_oversampled[metric], equal_var=False)\n",
    "    results.append({\n",
    "        \"Data\": \"cnn_all\",      # Because your filenames are \"all_*_cnn_metrics.csv\"\n",
    "        \"Metric\": metric,\n",
    "        \"T-stat\": round(t_stat, 4),\n",
    "        \"P-value\": round(p_val, 4)\n",
    "    })\n",
    "\n",
    "# 4) Convert to a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# 5) Pivot so each metric has its own T-stat and P-value columns, with only 1 row\n",
    "t_test_pivot = results_df.pivot_table(\n",
    "    index=\"Data\",       # single row for \"cnn_all\"\n",
    "    columns=\"Metric\",   # \"MCC\", \"F1_macro\", \"F1_weighted\"\n",
    "    values=[\"T-stat\",\"P-value\"]\n",
    ")\n",
    "\n",
    "# The columns are a multi-index: (\"T-stat\", \"MCC\"), (\"P-value\", \"MCC\"), ...\n",
    "# Flatten them\n",
    "new_cols = []\n",
    "for (stat_type, metric_name) in t_test_pivot.columns:\n",
    "    # e.g. (\"T-stat\", \"MCC\") -> \"MCC T-stat\"\n",
    "    new_cols.append(f\"{metric_name} {stat_type}\")\n",
    "t_test_pivot.columns = new_cols\n",
    "\n",
    "# Put index (Data) back as a column\n",
    "t_test_pivot.reset_index(inplace=True)\n",
    "\n",
    "# Optional: reorder columns\n",
    "desired_order = [\n",
    "    \"Data\",\n",
    "    \"MCC T-stat\", \"MCC P-value\",\n",
    "    \"F1_macro T-stat\", \"F1_macro P-value\",\n",
    "    \"F1_weighted T-stat\", \"F1_weighted P-value\",\n",
    "]\n",
    "t_test_pivot = t_test_pivot[desired_order]\n",
    "\n",
    "# 6) Save final CSV\n",
    "t_test_pivot.to_csv(\"t_test_results_svm.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2c229cd6ff87d19ea7d47541bfa1b62a250f965db58eb2430f02bd66bc83489c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
